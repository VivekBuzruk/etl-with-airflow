<!DOCTYPE html>
<html class="writer-html4" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Data Vault 2 &mdash; ETL Best Practices with Airflow v1.8</title><link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Data Vault with Big Data processes" href="datavault-bigdata.html" />
    <link rel="prev" title="Deployments" href="deployments.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> ETL Best Practices with airflow 1.8
          </a>
              <div class="version">
                1.8
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
    
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="principles.html">ETL principles</a></li>
<li class="toctree-l1"><a class="reference internal" href="gotchas.html">Gotcha’s</a></li>
<li class="toctree-l1"><a class="reference internal" href="great.html">What makes Airflow great?</a></li>
<li class="toctree-l1"><a class="reference internal" href="etlexample.html">ETL example</a></li>
<li class="toctree-l1"><a class="reference internal" href="hiveexample.html">Hive example</a></li>
<li class="toctree-l1"><a class="reference internal" href="datavault.html">Data vault</a></li>
<li class="toctree-l1"><a class="reference internal" href="monitoring.html">Monitoring</a></li>
<li class="toctree-l1"><a class="reference internal" href="platform.html">Building your own ETL platform</a></li>
<li class="toctree-l1"><a class="reference internal" href="ingestfile.html">Ingesting files</a></li>
<li class="toctree-l1"><a class="reference internal" href="tips.html">Tips</a></li>
<li class="toctree-l1"><a class="reference internal" href="deployments.html">Deployments</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Data Vault 2</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#about-datavault">About Datavault</a></li>
<li class="toctree-l2"><a class="reference internal" href="#overall-flow">Overall flow</a></li>
<li class="toctree-l2"><a class="reference internal" href="#staging-flow">Staging flow</a></li>
<li class="toctree-l2"><a class="reference internal" href="#data-vault-loading-flow">Data vault loading flow</a></li>
<li class="toctree-l2"><a class="reference internal" href="#end-dating">End dating</a></li>
<li class="toctree-l2"><a class="reference internal" href="#star-schema">Star Schema</a></li>
<li class="toctree-l2"><a class="reference internal" href="#future-considerations">Future considerations</a></li>
<li class="toctree-l2"><a class="reference internal" href="#data-issues">Data issues</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="datavault-bigdata.html">Data Vault with Big Data processes</a></li>
</ul>

    <br/>
    <br/>
    <a href="https://github.com/gtoonstra/etl-with-airflow"><img src="_images/GitHub-Mark-Light-32px.png">&nbsp;Go to Github</a>
  
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">ETL Best Practices with airflow 1.8</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Data Vault 2</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="data-vault-2">
<h1>Data Vault 2<a class="headerlink" href="#data-vault-2" title="Permalink to this headline">¶</a></h1>
<p>This is probably most elaborate example of how to use ETL with Apache Airflow.
As part of this exercise, let’s build an information mart on Google BigQuery through a DataVault
built on top of Hive. (Consequently, this example requires a bit more memory and may not fit in a simple machine).
We’re going to start a postgres instance that contains the airflow database and another
database for a (postgres port) of the adventureworks database often used by Microsoft.</p>
<p>The data will be staged into Hive and we’ll run Hive queries to populate the Data Vault
model. Optionally, if you have a Google account you’d like to try out, you can set up a
connection later on and load some flat tables into BigQuery out of the Data Vault as a final
part of this exercise; that will basically become our information mart.</p>
<p>Note that similar to the Hive example, I’m using a special build of the puckel docker airflow
container that contains the jar files for Hadoop, HDFS and Hive.</p>
<div class="admonition important">
<p class="first admonition-title">Important</p>
<p class="last">The default login for “Hue”, the interface for the Cloudera quickstart container running Hive
is cloudera/cloudera.</p>
</div>
<div class="admonition important">
<p class="first admonition-title">Important</p>
<p class="last">The resulting data was not 100% tested and validated because of the effort involved. It is possible
that queries contain mistakes or produce invalid data.</p>
</div>
<div class="admonition important">
<p class="first admonition-title">Important</p>
<p class="last">In order to see the BigQuery example you need to have your own google cloud project active
and set up your own service account that has at least google cloud storage and bigquery editing
permissions (or give it project editing rights). Then you need to make an “information_mart”
dataset in bigquery in that project and create a bucket name of your choice. After the bigquery
connection is created, change the project ID to your project ID on GCP (this is needed as the
‘default’ google project).</p>
</div>
<p>Finally, let’s re-test all the work we did against the ETL principles that I wrote about to see
if all principles are covered and identify what are open topics to cover for a full-circle solution.</p>
<div class="section" id="about-datavault">
<h2>About Datavault<a class="headerlink" href="#about-datavault" title="Permalink to this headline">¶</a></h2>
<p>In the <a class="reference internal" href="datavault.html"><span class="doc">Data vault</span></a> example, we explained some of the benefits of using a datavaulting methodology
to build your data warehouse and other rationales. Read up there for some of the core reasons why data vaulting
is such a useful methodology to use in the middle.</p>
<p>This example uses some other techniques and attempts to implement all the best practices associated with
data vaulting. The “2.0” refers to some improvements that have been made since the first version of the
methodology came out. One of the primary changes is the use of hashes as a means to improve the parallel
forward flow of the data going into the final information marts and intermediate processing. Hashing isn’t
necessarily straight-forward in all situations and you should prepare for making some practical design decisions there.</p>
</div>
<div class="section" id="overall-flow">
<h2>Overall flow<a class="headerlink" href="#overall-flow" title="Permalink to this headline">¶</a></h2>
<p>This is the general flow to get data from the OLTP system into (eventually) the information mart.
Here you can see how the Data Vault essentially fulfills the role of the Enterprise Data Warehouse
as described by Ralph Inmon, years ago.</p>
<img alt="_images/dataflow.jpeg" src="_images/dataflow.jpeg" />
<p>There are 3 dags in total. One dag starting with “<a href="#id1"><span class="problematic" id="id2">init_</span></a>” is just to bootstrap the example, you wouldn’t
see this DAG in a production situation, because you’d typically use CI tools + other tools to maintain your
schemas and you’d do the connection management in a different way. So ignore that DAG.</p>
<p>The ‘adventureworks’ DAG is the main point where five specific flows happen, which are marked by specific milestones:</p>
<ul class="simple">
<li>Staging</li>
<li>Applying staging to hubs</li>
<li>Applying staging to links</li>
<li>Applying staging to satellites</li>
<li>End dating satellites</li>
</ul>
<p>After the DAG completes, the data warehouse is in a new state and can be requeried to refresh downstream
data products, for example as would be done with the starschema DAG.</p>
</div>
<div class="section" id="staging-flow">
<h2>Staging flow<a class="headerlink" href="#staging-flow" title="Permalink to this headline">¶</a></h2>
<p>Staging is the process where you pick up data from a source system and load it into a ‘staging’ area
keeping as much as possible of the source data intact. “Hard” business rules may be applied,
for example changing the data type of an item from a string into a datetime, but you should avoid
splitting, combining or otherwise modifying the incoming data elements and leave that to a following step.
The latter are called “soft” business rules and are usually transformations related to interpretation
of the data. In short: operations where you may lose information should be avoided.</p>
<p>The staging area is temporary and I’m assuming delta loads are possible from the source system because of
a cdc solution being in place. If delta loads cannot be implemented due to a lack of proper CDC, then
a persistent staging area (PSA) should be set up, so you can generate delta loads from there and
identify the deletes. Both the latter and the CDC solution should be capable to detect deletes.</p>
<p>Our staging approach for all tables in the adventureworks dataset will be:</p>
<ol class="arabic simple">
<li>Clear out staging table (truncate or drop). In the case of Hive, we use a temporary table with a date and time tag at the end. This means that each particular staging table can only reference data from the current data load.</li>
<li>(optional) disable indexes. As we use Hive, this is not relevant, there are no indexes set.</li>
<li>Bulk Read source data in order. In this example we bulk read “everything” from the entire source system because there are no useful change date/times in the source data. In a real application, you’d divide the data through the “updated_dtm” field that the CDC system is setting.</li>
<li>Compute and apply system values:
* Load date
* Record source
* A sequence number, which defines the record ordering in the current batch.
* Hash for all business keys in a record. This is the record of the current table, but also business keys for all foreign keys into that table. The reason why this is important is because all surrogate sequences and primary keys that the source system may have should not have any significance in the data warehouse, unless they are also business keys for that table. This is the reason why I force the staging area to apply the hashes prior to loading it in the raw data vault.
* (optionally) a hash diff compiled from all or certain attributes in the source data that is used to perform change comparisons to identify duplicates, so we don’t load records twice.</li>
<li>Remove true duplicates</li>
<li>Insert records into staging table</li>
<li>(optional) rebuild indexes. Again, not relevant for this setup.</li>
</ol>
<p>Given the above operations, we see that we should be able to apply a very common pattern to each
source table that we need to ingest. The general strategy is that in the staging area, every record
of interest for the current date partition gets loaded. In those records, the record gets a
hash key assigned at the very least (even if that resolves to just a surrogate primary key) and
all foreign keys result in inner joins to other tables, so that we can generate the hash key for
the business keys in there. This is because the foreign keys will eventually convert to a link
of some sort and having the hash key ready in staging allows us to parallellize the following stages
as well. As a matter of fact, it feels wrong to resolve the hashes later. These lookups may have a higher
impact on the source system because of the extra joins for each table, but these lookups have to be made
‘somewhere’ and because I believe the source system is where the surrogate keys are relevant, it should be
resolved from there.</p>
<p>In the current implementation I’m using python code to apply the hashing, because it demonstrates that
hashing is possible even if the database engine doesn’t implement your hash algorithm of choice.</p>
<div class="admonition important">
<p class="first admonition-title">Important</p>
<p class="last">The adventureworks database has some serious design flaws and doesn’t expose a lot of useful
“natural” business keys that are so important in data vaulting. Because businesses have people that
talk about the data a lot, you should find a lot more references, identifiers and natural business keys
in a true database setup that is actually used by and for people. The main staging setup is done in the
“adventureworks_*.py” files, which reference the SQL files in the ‘sql’ folder. In the SQL, you’ll see the
construction of the natural business keys at that stage. The python operator picks up the generated string and
converts that into a hash using a hash function. The reason to do this per record is because a source database
system doesn’t necessarily have the right capabilities to do this.</p>
</div>
<p>There’s an important remark to make about “pre-hashing” business keys in the staging area. It means that the
decisions on what and how to hash are made in the staging area and there may be further issues downstream where
these design decisions can come into play. As the objective is to follow the methodology, we go along with
that and see where this takes us. If you feel unhappy about this, look into setting up a PSA, which will give you
the ability to reload the whole DV at a later stage because all the staging data is preserved.</p>
<p>Another important note: notice how we don’t specify what hive staging tables should look like. We’re simply
specifying what we want to see in the Hive table. Because Hive is “Schema On Read”, you can’t enforce nullability
either, so there’s no reason to set up a structured destination schema because nothing can be enforced about
it anyway.</p>
<p>Let’s look at the flow in more detail:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">args</span> <span class="o">=</span> <span class="p">{</span>
    <span class="o">....</span>
    <span class="c1"># We want to maintain chronological order when loading the datavault</span>
    <span class="s1">&#39;depends_on_past&#39;</span><span class="p">:</span> <span class="kc">True</span>
<span class="p">}</span>
<span class="o">...</span>

<span class="c1"># specify the purpose for each dag</span>
<span class="n">RECORD_SOURCE</span> <span class="o">=</span> <span class="s1">&#39;adventureworks.sales&#39;</span>

<span class="c1"># Use a dummy operator as a &quot;knot&quot; to synchronize staging loads</span>
<span class="n">staging_done</span> <span class="o">=</span> <span class="n">DummyOperator</span><span class="p">(</span>
    <span class="n">task_id</span><span class="o">=</span><span class="s1">&#39;staging_done&#39;</span><span class="p">,</span>
    <span class="n">dag</span><span class="o">=</span><span class="n">dag</span><span class="p">)</span>

<span class="c1"># A function helps to generalize the parameters,</span>
<span class="c1"># so we can just write 2-3 lines of code to get a</span>
<span class="c1"># table staged into our datavault</span>
<span class="k">def</span> <span class="nf">create_staging_operator</span><span class="p">(</span><span class="n">sql</span><span class="p">,</span> <span class="n">hive_table</span><span class="p">,</span> <span class="n">record_source</span><span class="o">=</span><span class="n">RECORD_SOURCE</span><span class="p">):</span>
    <span class="n">t1</span> <span class="o">=</span> <span class="n">StagePostgresToHiveOperator</span><span class="p">(</span>
        <span class="c1"># The SQL running on postgres</span>
        <span class="n">sql</span><span class="o">=</span><span class="n">sql</span><span class="p">,</span>
        <span class="c1"># Create and recreate a hive table with the &lt;name&gt;_yyyymmddthhmmss pattern</span>
        <span class="n">hive_table</span><span class="o">=</span><span class="n">hive_table</span> <span class="o">+</span> <span class="s1">&#39;_{{ts_nodash}}&#39;</span><span class="p">,</span>
        <span class="n">postgres_conn_id</span><span class="o">=</span><span class="s1">&#39;adventureworks&#39;</span><span class="p">,</span>
        <span class="n">hive_cli_conn_id</span><span class="o">=</span><span class="s1">&#39;hive_advworks_staging&#39;</span><span class="p">,</span>
        <span class="c1"># Create a destination table, drop and recreate it every run.</span>
        <span class="c1"># Because of the pattern above, we don&#39;t need truncates.</span>
        <span class="n">create</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">recreate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">record_source</span><span class="o">=</span><span class="n">record_source</span><span class="p">,</span>
        <span class="c1"># Specifying the &quot;load_dtm&quot; for this run</span>
        <span class="n">load_dtm</span><span class="o">=</span><span class="s1">&#39;{{execution_date}}&#39;</span><span class="p">,</span>
        <span class="c1"># A generalized name</span>
        <span class="n">task_id</span><span class="o">=</span><span class="s1">&#39;stg_</span><span class="si">{0}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">hive_table</span><span class="p">),</span>
        <span class="n">dag</span><span class="o">=</span><span class="n">dag</span><span class="p">)</span>

    <span class="c1"># Putting it in the flow...</span>
    <span class="n">t1</span> <span class="o">&gt;&gt;</span> <span class="n">staging_done</span>
    <span class="k">return</span> <span class="n">t1</span>

<span class="c1"># Example of the effort of staging a new table</span>
<span class="n">create_staging_operator</span><span class="p">(</span>
    <span class="n">sql</span><span class="o">=</span><span class="s1">&#39;staging/salesorderheader.sql&#39;</span><span class="p">,</span>
    <span class="n">hive_table</span><span class="o">=</span><span class="s1">&#39;salesorderheader&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Important design principles to focus on:</p>
<ul class="simple">
<li>Each staging table is tied to a processing run in airflow and is marked by its own YYYYMMDDTHHMMSS partition. The reason to include a time structure is to think ahead and ingest data in the data warehouse more frequently than once per day. Because we keep staging data separately this way, we don’t need to worry about multiple staging cycles in the same table and filter by load_dtm, except for getting the name of the table right. Doing it this allows us to continue to load data in staging even though we can’t perhaps (for some reason) load it into the DV yet.</li>
<li>“depends_on_past” is set to True because we want to force loading data into the datavault in chronological order. The data into staging isn’t a critical step, but since each sub pipeline also contains operators for loading the datavault, the whole dag by default is set to the same principle.</li>
<li>When everything was loaded, we can drop the temp staging table or decide to copy it to a partitioned PSA table.</li>
<li>New tables can be added by creating a query for it and 3 lines of code, which looks like a great generalization for this process. It is definitely possible to set up a template and generate the required tables from an input table to further ease this process.</li>
<li>Because of the previous point, the entire table staging process is very generic and predictable.</li>
<li>There are three distinct parallel processing phases as one would expect from the design of data vault.</li>
</ul>
</div>
<div class="section" id="data-vault-loading-flow">
<h2>Data vault loading flow<a class="headerlink" href="#data-vault-loading-flow" title="Permalink to this headline">¶</a></h2>
<p>Now that data is in staging, it is time to start loading the staging data into datavault. Use the “adventureworks_*” dags for that, there is one for each schema in the database. Here’s a diagram that demonstrates the strategy:</p>
<img alt="_images/loading_strategy.jpg" src="_images/loading_strategy.jpg" />
<p>An important design decision has been made in this process:</p>
<p><em>Getting the business key hashes for all foreign key is a challenge and I opted to generate all
hashes from the source database using INNER JOINs. The reason is that I’m assuming a CDC slave
database system that has no other load and good optimization for querying and joining data on subselects
of the driving table.</em></p>
<p>I think there are three possibilities to resolve this:</p>
<ul class="simple">
<li>Generate hashes for all primary+foreign keys from the source system (as in this implementation). The rationale is that surrogate sequence keys frequently used in an RDBMS should only have meaning within the context of that RDBMS, so it is important to apply business keys to business entities as soon as possible.</li>
<li>Generate hashes for those identified business keys you happen to come across and then use more elaborate joins on the data vault (even joining on satellites in cases).</li>
<li>Create a cache/lookup table for each source system in the staging area that then becomes an integral part of your data warehouse. The idea is to dissociate the surrogate key from the source system and convert that into a hash without adding significant load on the source system. The rationale is that the data warehouse needs the hash key in order to operate, but the source system has given all the data the DWH is asking for. The DWH itself should be responsible for caching and deliverying the hash key that is needed.</li>
</ul>
<p>This is a block template of code significant for the loading part:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">hubs_done</span> <span class="o">=</span> <span class="n">DummyOperator</span><span class="p">(</span>
    <span class="n">task_id</span><span class="o">=</span><span class="s1">&#39;hubs_done&#39;</span><span class="p">,</span>
    <span class="n">dag</span><span class="o">=</span><span class="n">dag</span><span class="p">)</span>
<span class="n">links_done</span> <span class="o">=</span> <span class="n">DummyOperator</span><span class="p">(</span>
    <span class="n">task_id</span><span class="o">=</span><span class="s1">&#39;links_done&#39;</span><span class="p">,</span>
    <span class="n">dag</span><span class="o">=</span><span class="n">dag</span><span class="p">)</span>
<span class="n">sats_done</span> <span class="o">=</span>  <span class="n">DummyOperator</span><span class="p">(</span>
    <span class="n">task_id</span><span class="o">=</span><span class="s1">&#39;sats_done&#39;</span><span class="p">,</span>
    <span class="n">dag</span><span class="o">=</span><span class="n">dag</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">create_hub_operator</span><span class="p">(</span><span class="n">hql</span><span class="p">,</span> <span class="n">hive_table</span><span class="p">):</span>
    <span class="n">t1</span> <span class="o">=</span> <span class="n">HiveOperator</span><span class="p">(</span>
        <span class="n">hql</span><span class="o">=</span><span class="n">hql</span><span class="p">,</span>
        <span class="n">hive_cli_conn_id</span><span class="o">=</span><span class="s1">&#39;hive_datavault_raw&#39;</span><span class="p">,</span>
        <span class="n">schema</span><span class="o">=</span><span class="s1">&#39;dv_raw&#39;</span><span class="p">,</span>
        <span class="n">task_id</span><span class="o">=</span><span class="n">hive_table</span><span class="p">,</span>
        <span class="n">dag</span><span class="o">=</span><span class="n">dag</span><span class="p">)</span>

    <span class="n">staging_done</span> <span class="o">&gt;&gt;</span> <span class="n">t1</span>
    <span class="n">t1</span> <span class="o">&gt;&gt;</span> <span class="n">hubs_done</span>
    <span class="k">return</span> <span class="n">t1</span>

<span class="k">def</span> <span class="nf">create_link_operator</span><span class="p">(</span><span class="n">hql</span><span class="p">,</span> <span class="n">hive_table</span><span class="p">):</span>
    <span class="n">t1</span> <span class="o">=</span> <span class="n">HiveOperator</span><span class="p">(</span>
        <span class="n">hql</span><span class="o">=</span><span class="n">hql</span><span class="p">,</span>
        <span class="n">hive_cli_conn_id</span><span class="o">=</span><span class="s1">&#39;hive_datavault_raw&#39;</span><span class="p">,</span>
        <span class="n">schema</span><span class="o">=</span><span class="s1">&#39;dv_raw&#39;</span><span class="p">,</span>
        <span class="n">task_id</span><span class="o">=</span><span class="n">hive_table</span><span class="p">,</span>
        <span class="n">dag</span><span class="o">=</span><span class="n">dag</span><span class="p">)</span>

<span class="c1"># hubs</span>
<span class="n">create_hub_operator</span><span class="p">(</span><span class="s1">&#39;loading/hub_salesorder.hql&#39;</span><span class="p">,</span> <span class="s1">&#39;hub_salesorder&#39;</span><span class="p">)</span>
<span class="o">....</span>

<span class="c1"># links</span>
<span class="n">create_link_operator</span><span class="p">(</span><span class="s1">&#39;loading/link_salesorderdetail.hql&#39;</span><span class="p">,</span> <span class="s1">&#39;link_salesorderdetail&#39;</span><span class="p">)</span>
<span class="o">....</span>
</pre></div>
</div>
<p>Each operator links to the dummy, which gives us the synchronization points.
Because links may have dependencies outside each functional area (determined by the schema)
some further synchronization is required there.</p>
<p>The loading code follows the same principles as the Data Vault 2.0 default stanzas:</p>
<p>Loading a hub is concerned about creating an ‘anchor’ around which elements referring to a business
entity resolve. Notice the absence of “record_source” check, so whichever system first sees this
business key will win the record inserted here.:</p>
<div class="highlight-SQL notranslate"><div class="highlight"><pre><span></span>INSERT INTO TABLE dv_raw.hub_product
SELECT DISTINCT
    p.hkey_product,
    p.record_source,
    p.load_dtm,
    p.productnumber
FROM
    advworks_staging.product_{{ts_nodash}} p
WHERE
    p.productnumber NOT IN (
        SELECT hub.productnumber FROM dv_raw.hub_product hub
    )
</pre></div>
</div>
<p>Loading a link is basically tying some hubs together. Any details related to the characteristics of the relationship are kept in a satellite table tied to the link.</p>
<div class="highlight-SQL notranslate"><div class="highlight"><pre><span></span>INSERT INTO TABLE dv_raw.link_salesorderdetail
SELECT DISTINCT
    sod.hkey_salesorderdetail,
    sod.hkey_salesorder,
    sod.hkey_specialoffer,
    sod.hkey_product,
    sod.record_source,
    sod.load_dtm,
    sod.salesorderdetailid
FROM
           advworks_staging.salesorderdetail_{{ts_nodash}} sod
WHERE
    NOT EXISTS (
        SELECT
                l.hkey_salesorderdetail
        FROM    dv_raw.link_salesorderdetail l
        WHERE
                l.hkey_salesorder = sod.hkey_salesorder
        AND     l.hkey_specialoffer = sod.hkey_specialoffer
        AND     l.hkey_product = sod.hkey_product
    )
</pre></div>
</div>
<p>Loading satellite is the point where chronological ordering becomes truly important. If we don’t get the load cycles in chronological order for hubs and links then the “load_dtm” for them will be wrong, but functionally the data vault should keep operating. Why is this only relevant for satellites?  Because hubs and links do not have ‘rate-of-change’. The links document relationships, but these do not change over time, except for their supposed effectivity. Hubs document the presence of business keys, but these do not change over time, except for their supposed effectivity. Only satellites have a rate-of-change associated with them, which is why they have start and end dates. It is possible that a business key or relation gets deleted in the source system. In our our datavault we’d like to maintain the data there (we never delete except for corruption / resolving incidents). The way how that is done is through “effectivity” tables, which are start/end dates in a table connected to the hub or link that record over which time that hub or link should be active.</p>
<p>For satellites, the chronological ordering determines the version of the entity at a specific time, so it affects what the most current version would look like now. This is why they have to be loaded in chronological order, because if they were not, the last active record would be different and the active periods would probably look skewed. Another objective for loading it in chronological order is to eliminate true duplicates; if the records come in fast and do not have a chronological order than either true duplicates are not always detected or un-true duplicates are detected and records get eliminated.</p>
<p>Splitting a satellite is a common practice to record data that has different rates of change. For example, if a table has 40 columns as 20 columns change rapidly and 20 more slowly, then if we were to keep everything in the same table, we’d accumulate data twice as fast. By splitting it into 2 separate tables we can keep the detailed changes to a minimum. This is the typical stanza for loading a satellite. Pay attention to how in Hive you can’t specify destination columns. If you keep staging data in the same table you’d also have an additional WHERE clause that specifies <cite>load_dtm = xxxxx</cite>.</p>
<div class="highlight-SQL notranslate"><div class="highlight"><pre><span></span>INSERT INTO TABLE dv_raw.sat_salesorderdetail
SELECT DISTINCT
      so.hkey_salesorderdetail
    , so.load_dtm
    , NULL
    , so.record_source
    , so.carriertrackingnumber
    , so.orderqty
    , so.unitprice
    , so.unitpricediscount
FROM
                advworks_staging.salesorderdetail_{{ts_nodash}} so
LEFT OUTER JOIN dv_raw.sat_salesorderdetail sat ON (
                sat.hkey_salesorderdetail = so.hkey_salesorderdetail
            AND sat.load_end_dtm IS NULL)
WHERE
   COALESCE(so.carriertrackingnumber, &#39;&#39;) != COALESCE(sat.carriertrackingnumber, &#39;&#39;)
OR COALESCE(so.orderqty, &#39;&#39;) != COALESCE(sat.orderqty, &#39;&#39;)
OR COALESCE(so.unitprice, &#39;&#39;) != COALESCE(sat.unitprice, &#39;&#39;)
OR COALESCE(so.unitpricediscount, &#39;&#39;) != COALESCE(sat.unitpricediscount, &#39;&#39;)
</pre></div>
</div>
</div>
<div class="section" id="end-dating">
<h2>End dating<a class="headerlink" href="#end-dating" title="Permalink to this headline">¶</a></h2>
<p>The hubs and links do not contain start and end dates, because they record relationships, even relationships that were valid at some point in time. If you need to cater for employees joining, leaving and joining again for example, you should use an “effectivity” table connected to the link or hub to cater for that.</p>
<p>The satellites do have validity dates, because you can have different versions of those. The way how you apply those can differ a bit, because you may not always have the required source data if you don’t have a change data capture setup. Then you’d only ever
see the last version of a record or the records that the source system decided to maintain as history. The date you’d apply as start/end date could then differ.</p>
<p>It’s always very important to maintain the “load_dtm” and “load_end_dtm” separately as well, because you’d use that to identify
data from batches that may have failed for example. If you maintain it, you can always remove data for an entire batch and reload it into the data vault.</p>
<p>The process of end dating is to apply end dates to records in the satellite tables. For Hive, because we can’t run updates, we’ll copy the data to a temp table and then copy it back to the original. We can use windowing functions like LAG/LEAD and PARTITION statements, so we use that to look ahead by one row for each partition to look up the next start date and apply that for the end date.</p>
<p>When a record for a partition has a NULL end_dtm, then it means it’s the active record. You could choose to explicitly indicate the active record too.</p>
</div>
<div class="section" id="star-schema">
<h2>Star Schema<a class="headerlink" href="#star-schema" title="Permalink to this headline">¶</a></h2>
<p>The star schema is built with the help of some multi-join queries. The dimensions are built up first and then
the fact information is built on top of the dimensions. You don’t need to build the dimensions with one single query,
it’s obviously permissible to run a multi-stage pipeline to get the dimensions built.</p>
<p>Here’s a [good article](<a class="reference external" href="https://towardsdatascience.com/a-beginners-guide-to-data-engineering-part-ii-47c4e7cbda71">https://towardsdatascience.com/a-beginners-guide-to-data-engineering-part-ii-47c4e7cbda71</a>) on how Hive is used with dimensional data:</p>
<p>The dimensions in this example use the original hash key as main key for the dimensional entity and in the case of slowly changing dimensions (where dates are applicable and important), it tags the start date on top of the hash key of the entity to derive a new dimensional key.</p>
<p>The fact is built on top of one of the measures of interest. Usually, you’ll find that these are link tables, because they often
link the entities in a context together. For example, the orderlineitem is a link table, because it links the order data with
sold product data, applied discounts and some other data depending on the business.</p>
<p>The fact table can rapidly become complex if there is a lot of data to link together. Similar to building the dimension models, consider splitting up the complex queries by using temp tables that are joined together afterwards to compose the full picture.</p>
<p>The example shows how to generate a star schema from scratch without applying incremental changes. If your data vault grows
a bit large than regenerating it from scratch will be very costly to do, if not impossible within the given timeframe. Refer to the article in this section for a method that shows how to copy the dimension of the day before and union that to new records in the dimension.</p>
</div>
<div class="section" id="future-considerations">
<h2>Future considerations<a class="headerlink" href="#future-considerations" title="Permalink to this headline">¶</a></h2>
<p>What is not shown in this example and which should be considered in real world scenarios:</p>
<ul class="simple">
<li>Dealing with “delete” records in data sources. You’d typically apply these as ‘effectivity’ records on hubs and links.</li>
<li>Dealing with “record order” correctly. The current post-update scripts that do the end-dating assume there is one record per entity in the time interval, but there may be multiple. Make sure that the end dating script applies the end date to the records in the correct order and ensure the most recent record comes out on top with “NULL” applied in the end_dtm.</li>
<li>Dealing with (potentially) large output (larger than 2GB). At the moment the worker reads in all the data in memory and then copies it again into a JSON structure.</li>
</ul>
<p>There are ways to output data to multiple files in a single statement using a “named pipe” on the worker itself. The named pipe serves the function as a splitter. You’d then start a “linux split” command on the worker reading from the named pipe (which looks just like a file, except it cannot seek in the stream). The split command takes the input and splits the data into separate files of a particular maximum size or maximum number of lines. If you do this to a particular temporary directory of interest, you can then upload the files to GCP from that directory in one easy operation, either through the gsutil command or an operator.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">NamedTemporaryDir</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;export_&#39;</span> <span class="k">as</span> <span class="n">tmp_dir</span><span class="p">:</span>
    <span class="n">fifo_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tmp_dir</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="s1">&#39;fifopipe&#39;</span><span class="p">)</span>
    <span class="n">os</span><span class="o">.</span><span class="n">mkfifo</span><span class="p">(</span><span class="n">fifo_path</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">Popen</span><span class="p">([</span><span class="s1">&#39;split&#39;</span><span class="p">,</span><span class="s1">&#39;--numlines&#39;</span><span class="p">,</span><span class="s1">&#39;100000&#39;</span><span class="p">,</span><span class="s1">&#39;-&#39;</span><span class="p">,</span><span class="n">prefix</span><span class="p">])</span>
    <span class="n">hiveHook</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="o">&lt;</span><span class="n">query</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">fifo_path</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
    <span class="n">p</span><span class="o">.</span><span class="n">communicate</span><span class="p">()</span>
    <span class="n">os</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">fifo_path</span><span class="p">)</span>
    <span class="n">datafiles</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">listdir</span><span class="p">(</span><span class="n">tmp_dir</span><span class="p">)</span> <span class="k">if</span> <span class="n">isfile</span><span class="p">(</span><span class="n">join</span><span class="p">(</span><span class="n">tmp_dir</span><span class="p">,</span> <span class="n">f</span><span class="p">))</span>
                 <span class="ow">and</span> <span class="n">f</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">prefix</span><span class="p">)]</span>
    <span class="k">for</span> <span class="n">data_file</span> <span class="ow">in</span> <span class="n">datafiles</span><span class="p">:</span>
        <span class="n">remote_name</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="si">{0}</span><span class="s1">.csv&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">data_file</span><span class="p">)</span>
        <span class="n">gcp_hook</span><span class="o">.</span><span class="n">upload</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bucket</span><span class="p">,</span> <span class="n">remote_name</span><span class="p">,</span> <span class="n">data_file</span><span class="p">)</span>
</pre></div>
</div>
<p>Or use a call to gsutil to perform a data upload in parallel.</p>
</div>
<div class="section" id="data-issues">
<h2>Data issues<a class="headerlink" href="#data-issues" title="Permalink to this headline">¶</a></h2>
<p>The adventure works database isn’t the best designed OLTP database ever. Throughout querying and working with the data I found the following data issues:</p>
<ul class="simple">
<li>Address missed a row in the destination data. “Everett” has two records in address and the only difference is the stateprovinceid. Either the boundaries shifted or there was a correction made in the data.</li>
<li>There are some 700 personid’s missing for “customer” in the source data. Looks like it malfunctioned and never got fixed?</li>
<li>209 products do not have sub categories, so I allowed that to be NULLable.</li>
<li>There can be multiple sales reasons for a salesorder (as per the design). There is a hard business rule when constructing the dim_salesorder which picks the first sales reason by sales reason name ordered ascending to apply to the dimension.</li>
<li>Because of an incomplete business key in address multiple records get created in the dim_address table (4 in total). This table gets cartesian joined again to populate the fact, which leads to a total of 16 records too many for a specific salesorder (‘16B735DD67E11B7F9028EF9B4571CF25D1017CF1’)</li>
<li>Data has not been checked for consistency, correctness and bugs may exist anywhere in the code.</li>
</ul>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="deployments.html" class="btn btn-neutral float-left" title="Deployments" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="datavault-bigdata.html" class="btn btn-neutral float-right" title="Data Vault with Big Data processes" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2016, Gerard Toonstra.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>